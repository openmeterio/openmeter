address: 127.0.0.1:8888

telemetry:
  log:
    level: debug

#ingest:
#  kafka:
#    # To enable stats reporting set this value to >=5s.
#    # Setting this value to 0 makes reporting explicitly disabled.
#    statsInterval: 5s
#    # Set IP address family used for communicating with Kafka cluster
#    brokerAddressFamily: v4
#    # Use this configuration parameter to define how frequently the local metadata cache needs to be updated.
#    # It cannot be lower than 10 seconds.
#    topicMetadataRefreshInterval: 1m
#    # Use this config parameter to enable TCP keep-alive in order to prevent the Kafka broker to close idle network connection.
#    socketKeepAliveEnabled: true
#    # Set list of debug contexts to enable for librdkafka
#    # See: https://github.com/confluentinc/librdkafka/blob/master/INTRODUCTION.md#debug-contexts
#    debugContexts:
#      - broker
#      - topic
#  # The maximum number of entries stored in topic cache at a time which after the least recently used is evicted.
#  # Setting it to 0 makes the cache size unlimited.
#  cacheSize: 250
#  # CacheTTL stores maximum time an entries is kept in cache before being evicted.
#  # Setting it to 0 disables cache entry expiration.
#  cacheTTL: 5m

# dedupe:
#   enabled: true
#   driver: redis
#   config:
#     address: 127.0.0.1:6379
#     database: 0
#     expiration: 768h # 32d

# Entitlements
entitlements:
  enabled: true

billing:
  enabled: true

apps:
  enabled: true

# System event generation
events:
  enabled: false
# systemEvents:
#   enabled: true
#   topic: om_sys.api_events
#   autoProvision:
#     enabled: true
#     partitions: 4
# ingestEvents:
#   enabled: true
#   topic: om_sys.ingest_events
#   autoProvision:
#     enabled: true
#     partitions: 8

# Consumer portal
# portal:
#   enabled: true
#   tokenSecret: this-isnt-secure

postgres:
  url: postgres://postgres:postgres@localhost:5432/postgres?sslmode=disable
  autoMigrate: ent # Runs migrations as part of the service startup, valid values are: ent, migration, false

meters:
  # Sample meter to count API requests
  - slug: api_requests_total        # Unique identifier for the meter
    description: API Requests
    eventType: request              # Filter events by type
    aggregation: COUNT              # Aggregation method: COUNT, SUM, etc.
    groupBy:
      method: $.method              # HTTP Method: GET, POST, etc.
      route: $.route                # Route: /products/:product_id

  # Sample meter to count LLM Token Usage
  - slug: tokens_total
    description: AI Token Usage
    eventType: prompt               # Filter events by type
    aggregation: SUM
    valueProperty: $.tokens         # JSONPath to parse usage value
    groupBy:
      model: $.model                # AI model used: gpt4-turbo, etc.
      type: $.type                  # Prompt type: input, output, system

  # Sample meter to count workload runtime
  - slug: workload_runtime_duration_seconds
    description: Workload runtime duration in seconds
    eventType: workload                       # Filter events by type
    aggregation: SUM
    valueProperty: $.duration_seconds         # JSONPath to parse usage value
    groupBy:
      region: $.region                        # Cloud region: us-east-1, etc.
      zone: $.zone                            # Cloud zone: us-east-1a, etc.
      instance_type: $.instance_type          # Instance type: t2.micro, etc.

notification:
  enabled: true
#  webhook:
#    eventTypeRegistrationTimeout: 30s
#    skipEventTypeRegistrationOnError: false

svix:
  apiKey: secret
  serverURL: http://localhost:8071

#sink:
#  kafka:
#    brokers: 127.0.0.1:9092,127.0.0.2:9092
#    securityProtocol: SASL_SSL
#    saslMechanisms: PLAIN
#    saslUsername: user
#    saslPassword: pass
#    # To enable stats reporting set this value to >=5s.
#    # Setting this value to 0 makes reporting explicitly disabled.
#    statsInterval: 5s
#    # Set IP address family used for communicating with Kafka cluster
#    brokerAddressFamily: v4
#    # Use this configuration parameter to define how frequently the local metadata cache needs to be updated.
#    # It cannot be lower than 10 seconds.
#    topicMetadataRefreshInterval: 1m
#    # Use this config parameter to enable TCP keep-alive in order to prevent the Kafka broker to close idle network connection.
#    socketKeepAliveEnabled: true
#    # Set list of debug contexts to enable for librdkafka
#    # See: https://github.com/confluentinc/librdkafka/blob/master/INTRODUCTION.md#debug-contexts
#    debugContexts:
#      - broker
#      - topic
#    # Consumer/Producer identifier
#    clientID: kafka-client-1
#    # Consumer group identifier
#    consumerGroupID: consumer-group
#    # Static membership identifier in consumer group
#    consumerGroupInstanceID: consumer-group-1
#    # Consumer group session and failure detection timeout.
#    # The consumer sends periodic heartbeats (heartbeatInterval) to indicate its liveness to the broker.
#    #	If no hearts are received by the broker for a group member within the session timeout,
#    #	the broker will remove the consumer from the group and trigger a rebalance.
#    sessionTimeout: 5m
#    # Consumer group session keepalive heartbeat interval
#    heartbeatInterval: 3s
#    # Automatically and periodically commit offsets in the background
#    enableAutoCommit: true
#    # Automatically store offset of last message provided to application.
#    # The offset store is an in-memory store of the next offset to (auto-)commit for each partition.
#    enableAutoOffsetStore: false
#    # AutoOffsetReset defines the action to take when there is no initial offset in offset store or the desired offset is out of range:
#    #	* "smallest","earliest","beginning": automatically reset the offset to the smallest offset
#    #	* "largest","latest","end": automatically reset the offset to the largest offset
#    #	* "error":  trigger an error (ERR__AUTO_OFFSET_RESET) which is retrieved by consuming messages and checking 'message->err'.
#    autoOffsetReset: "error"
#    # partitionAssignmentStrategy defines one or more partition assignment strategies.
#    # The elected group leader will use a strategy supported by all members of the group to assign partitions to group members.
#    # If there is more than one eligible strategy, preference is determined by the order of this list (strategies earlier in the list have higher priority).
#    # Cooperative and non-cooperative (eager) strategies must not be mixed.
#    partitionAssignmentStrategy:
#      - range
#      - roundrobin
#      - cooperative-sticky
